---
title: "Web scraping"
author: "Prof. Maria Tackett"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta199-slides.css"
    logo: img/sta199-sticker-icon.png
    lib_dir: libs/font-awesome
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%" 
      ratio: "16:9"
---

layout: true

<div class="my-footer">
<span>
<a href="http://datasciencebox.org" target="_blank">datasciencebox.org</a>
</span>
</div> 

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message =F, 
                      fig.align = "center")

library(tidyverse)
library(scales)
library(here)
library(rvest)
library(DT)
xaringanExtra::use_panelset()
```

---

class: middle center

## [Click for PDF of slides](24-web-scraping.pdf)

---

class: middle

# Scraping the web

---

## Scraping the web: what? why?

- Increasing amount of data is available on the web
--

- These data are provided in an unstructured format: you can always copy&paste, 
but it's time-consuming and prone to errors

--

- Web scraping is the process of extracting this information automatically and transform it into a structured dataset

--

- Two different scenarios:
    - Screen scraping: extract data from source code of website, with html 
    parser (easy) or regular expression matching (less easy).
    - Web APIs (application programming interface): website offers a set of 
    structured http requests that return JSON or XML files.

---

class: middle

# Web Scraping with rvest

---

## Hypertext Markup Language (HTML)

.midi[
- HTML describes the structure of a web page; your browser interprets the 
  structure and contents and displays the results.
  
- The basic building blocks include elements, tags, and attributes.
    - an element is a component of an HTML document
    - elements contain tags (start and end tag)
    - attributes provide additional information about HTML elements
]

---

## Hypertext Markup Language (HTML)

```{r echo = F, out.width  = "70%"}
knitr::include_graphics("img/24/html-structure.png")
```

---

## Simple HTML document

```html
<html>
<head>
<title>Web Scraping</title>
</head>
<body>

<h1>Using rvest</h1>
<p>To get started...</p>

</body>
</html>
```

--

We can visualize this in a tree-like structure.

---

## HTML tree-like structure

```{r echo = F, out.width = "60%"}
knitr::include_graphics("img/24/html-tree.png")
```

---

class: middle 

### If we have access to an HTML document, then how can we easily extract information and get it in a format that's useful for anlaysis? 

---

## rvest

.pull-left[
- The **rvest** package makes basic processing and manipulation of HTML data straight forward
- It's designed to work with pipelines built with `%>%`
]
.pull-right[
```{r echo=FALSE,out.width=230,fig.align="right"}
knitr::include_graphics("img/24/rvest.png")
```
]

---

## Core rvest functions

- `read_html`   - Read HTML data from a url or character string
- `html_node `  - Select a specified node from HTML document
- `html_nodes`  - Select specified nodes from HTML document
- `html_table`  - Parse an HTML table into a data frame
- `html_text`   - Extract tag pairs' content
- `html_name`   - Extract tags' names
- `html_attrs`  - Extract all of each tag's attributes
- `html_attr`   - Extract tags' attribute value by name

---

## Example: simple_html

Let's suppose we have the following HTML document from the example website with the URL `simple_html`

```html 
<html>
<head>
<title>Web Scraping</title>
</head>
<body>
<h1>Using rvest</h1>
<p>To get started...</p>
</body>
</html>
```

```{r echo = F}
simple_html <- "<html>
<head>
<title>Web Scraping</title>
</head>
<body>
<h1>Using rvest</h1>
<p>To get started...</p>
</body>
</html>"
```

---

## HTML in R

Read in the document with `read_html()`.

```{r}
page <- read_html(simple_html) #replace with URL in practice
```

--

What does this look like?

--

```{r}
page
```

---

## Subset with `html_nodes()`

Let's extract the highlighted component below.

.small[
```{r eval=FALSE}
<html>
<head>
<title>Web Scraping</title>
</head>
<body>

{{<h1>Using rvest</h1>}}
<p>To get started...</p>

</body>
</html>
```
]
--

.midi[
```{r}
h1_nodes <-page %>%
  html_nodes(css = "h1")
h1_nodes
```
]
---

## Extract contents and tag name

Let's extract "Using rvest" and `h1`.

.small[
```{r eval=FALSE}
<html>
<head>
<title>Web Scraping</title>
</head>
<body>

{{<h1>Using rvest</h1>}}
<p>To get started...</p>

</body>
</html>
```
]
--

.pull-left[
.midi[
```{r}
h1_nodes %>% 
  html_text()
```
]
]

.pull-right[
.midi[
```{r}
h1_nodes %>% 
  html_name()
```
]
]
---

## Scaling up

Most HTML documents are not as simple as what we just examined. There may be tables, hundreds of links, paragraphs of text, and more. Naturally, we may wonder:

1. How do we handle larger HTML documents? 

2. How do we know what to provide to `css` in function `html_nodes()` when
   we attempt to subset the HTML document?
   
3. Are these functions in `rvest` vectorized? For instance, are we able to get 
   all the content in the `td` tags on the slide that follows?

In Chrome, you can view the HTML document associated with a web page by going
to `View > Developer > View Source`.
---

## SelectorGadget

.pull-left[
- Open source tool that eases CSS selector generation and discovery
- Easiest to use with the [Chrome Extension](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb) 
- Find out more on the [SelectorGadget vignette](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html)
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("img/24/selector-gadget.png")
```
]

---

## Using the SelectorGadget

.pull-left[
- .midi[Click on the app logo next to the search bar. A box will open in the bottom right of the website.]
- .midi[Click on a page element (it will turn green), SelectorGadget will generate a minimal CSS selector for that element, and will highlight (yellow) everything that is matched by the selector.]
]

.pull-right[
```{r echo=FALSE,fig.align="center",out.height=250}
knitr::include_graphics("img/24/selector-gadget.gif")
```
]


- .midi[Click on a highlighted element to remove it from the selector (red), or click on an unhighlighted element to add it to the selector.]

- .midi[Through this process of selection and rejection, SelectorGadget helps you come up with the appropriate CSS selector for your needs.]

---

class: middle

# Top 250 movies on IMDB

---

## Top 250 movies on IMDB

Take a look at the source code, look for the tag `table` tag:
<br>
http://www.imdb.com/chart/top

```{r echo = F, out.width = "60%"}
knitr::include_graphics("img/24/imdb_top_250.png")
```

---

## First check if you're allowed!

```{r warning=FALSE}
library(robotstxt)
paths_allowed("http://www.imdb.com")
```

vs. e.g.

```{r warning=FALSE}
paths_allowed("http://www.facebook.com")
```

---

## Select and format pieces

.small[
```{r message=FALSE, cache=TRUE}
page <- read_html("http://www.imdb.com/chart/top")
```
]

--

.small[
```{r message=FALSE, cache=TRUE}
titles <- page %>%
  html_nodes(".titleColumn a") %>%
  html_text()
```
]

--

.small[
```{r message=FALSE, cache=TRUE}
years <- page %>%
  html_nodes(".secondaryInfo") %>%
  html_text() %>%
  str_replace("\\(", "") %>% # remove (
  str_replace("\\)", "") %>% # remove )
  as.numeric()
```
]

--

.small[
```{r message=FALSE, cache=TRUE}
scores <- page %>%
  html_nodes("#main strong") %>%
  html_text() %>%
  as.numeric()
```
]

--

.small[
```{r message=FALSE, cache=TRUE}
imdb_top_250 <- tibble(
  title = titles, 
  year = years, 
  score = scores)
```
]

---

```{r}
imdb_top_250
```

---
.small[
```{r}
imdb_top_250 %>%
  DT::datatable(options(list(dom = "p"))) #<<
```
]

---

## Clean up / enhance

May or may not be a lot of work depending on how messy the data are

**See if you like what you got:**

.midi[
```{r}
glimpse(imdb_top_250)
```
]

--

**Add a variable for rank**

.midi[
```{r}
imdb_top_250 <- imdb_top_250 %>%
  mutate(rank = 1:nrow(imdb_top_250))
```
]

---

.small[
```{r}
imdb_top_250 %>%
  DT::datatable(options(list(dom = "p")), height = 350)
```
]

---

## Analyze

.question[
How would you go about answering this question: Which 1995 movies made the list?
]

--

.midi[
```{r}
imdb_top_250 %>% 
  filter(year == 1995)
```
]

---

## Analyze

.question[
How would you go about answering this question: Which years have the most movies on the list?
]

--

.midi[
```{r}
imdb_top_250 %>% 
  group_by(year) %>%
  summarise(total = n()) %>%
  arrange(desc(total)) %>%
  head(5)
```
]

---

.question[
How would you go about creating this visualization: Visualize the average yearly score for movies that made it on the top 250 list over time.
]

.panelset[
.panel[.panel-name[Plot]
```{r ref.label = "imdb-plot", echo = FALSE, warning = FALSE, fig.width = 8, fig.height = 4}
```
]

.panel[.panel-name[Code]
```{r imdb-plot, fig.show = "hide"}
imdb_top_250 %>% 
  group_by(year) %>%
  summarise(avg_score = mean(score)) %>%
  ggplot(aes(y = avg_score, x = year)) +
    geom_point() +
    geom_smooth(method = "lm") +
    labs(x = "Year", y = "Average score")
```
]
]

---

## Potential challenges

- Unreliable formatting at the source
- Data broken into many pages
- ...

.question[
Compare the display of information at [raleigh.craigslist.org/search/apa](https://raleigh.craigslist.org/search/apa) to the list on the IMDB top 250 list. What challenges can you foresee in scraping a list of the available apartments?
]
